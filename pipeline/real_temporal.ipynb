{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='NPE'\n",
    "dataset='ml-100k'\n",
    "\n",
    "config_dict = {\n",
    "    'eval_args': {\n",
    "        \"order\": \"TO\",\n",
    "        \"split\": {\"RS\": [0.8, 0.1, 0.1]},\n",
    "        \"group_by\": None\n",
    "    },\n",
    "    'train_neg_sample_args': None\n",
    "}\n",
    "\n",
    "config = Config(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    config_dict=config_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(config)\n",
    "# train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "# model_type = config[\"MODEL_TYPE\"]\n",
    "# built_datasets = dataset.build()\n",
    "# train_dataset, valid_dataset, test_dataset = built_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.config[\"normalize_field\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.float_like_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = \"timestamp\"\n",
    "\n",
    "assert field in dataset.fields(), f\"Dataset not existed field '{field}'\"\n",
    "\n",
    "for feat in dataset.field2feats(field):\n",
    "    break\n",
    "\n",
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.field2feats(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.inter_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = 'user_id'\n",
    "# grouped_inter_feat_index = dataset._grouped_index(dataset.inter_feat[group_by].numpy())\n",
    "\n",
    "dataset.inter_feat[group_by].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement time cutoff Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import importlib\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from recbole.data.dataloader import *\n",
    "from recbole.sampler import KGSampler, Sampler, RepeatableSampler\n",
    "from recbole.utils import ModelType, ensure_dir, get_local_time, set_color\n",
    "from recbole.utils.argument_list import dataset_arguments\n",
    "from recbole.data.dataset import Dataset\n",
    "from recbole.utils import (\n",
    "    FeatureType,\n",
    "    set_color,\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: HoangLe [Jun-09]: How to replace config[\"MODEL_TYPE\"] to TimeCutoffDataset\n",
    "\n",
    "class TimeCutoffDataset(Dataset):\n",
    "    def __init__(self, config):\n",
    "        self.timestamp_max, self.timestamp_min = 0., 0.\n",
    "        # NOTE: HoangLe [Jun-09]: May allow the user to specify the timestamp field in config\n",
    "        self.field_timestamp = \"timestamp\"\n",
    "\n",
    "        # Check if timestamp field is available\n",
    "        assert field in dataset.fields(), f\"Dataset not existe field '{field}'\"\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "    def _normalize(self):\n",
    "\n",
    "        # Extract max-min of field 'timestamp'\n",
    "        feat_timestamp = self.field2feats(self.field_timestamp)[0]\n",
    "        assert feat_timestamp and self.field_timestamp in feat_timestamp, f\"Feat not exist field '{self.field_timestamp}'\"\n",
    "\n",
    "        self.timestamp_max = np.max(feat_timestamp[self.field_timestamp])\n",
    "        self.timestamp_min = np.min(feat_timestamp[self.field_timestamp])\n",
    "\n",
    "        return super()._normalize()\n",
    "\n",
    "    def _fill_nan(self):\n",
    "        \"\"\"Missing value imputation.\n",
    "\n",
    "        For fields with type :obj:`~recbole.utils.enum_type.FeatureType.TOKEN`, missing value will be filled by\n",
    "        ``[PAD]``, which indexed as 0.\n",
    "\n",
    "        For fields with type :obj:`~recbole.utils.enum_type.FeatureType.FLOAT`, missing value will be filled by\n",
    "        the average of original data.\n",
    "\n",
    "        Note:\n",
    "            This is similar to the recbole's original implementation. The difference is the change in inplace operation to suit the pandas 3.0\n",
    "        \"\"\"\n",
    "        self.logger.debug(set_color(\"Filling nan\", \"green\"))\n",
    "\n",
    "        for feat_name in self.feat_name_list:\n",
    "            feat = getattr(self, feat_name)\n",
    "            for field in feat:\n",
    "                ftype = self.field2type[field]\n",
    "                if ftype == FeatureType.TOKEN:\n",
    "                    feat[field] = feat[field].fillna(value=0)\n",
    "                elif ftype == FeatureType.FLOAT:\n",
    "                    feat[field] = feat[field].fillna(value=feat[field].mean())\n",
    "                else:\n",
    "                    dtype = np.int64 if ftype == FeatureType.TOKEN_SEQ else np.float64\n",
    "                    feat[field] = feat[field].apply(\n",
    "                        lambda x: (\n",
    "                            np.array([], dtype=dtype) if isinstance(x, float) else x\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    def build(self):\n",
    "        self._change_feat_format()\n",
    "\n",
    "        if self.benchmark_filename_list is not None:\n",
    "            super().build()\n",
    "\n",
    "        # ordering\n",
    "        ordering_args = self.config[\"eval_args\"][\"order\"]\n",
    "        if ordering_args == \"TO\":\n",
    "            self.sort(by=self.time_field)\n",
    "        else:\n",
    "            raise AssertionError(\"The ordering_method must be 'TO.\")\n",
    "\n",
    "        # splitting & grouping\n",
    "        split_args = self.config[\"eval_args\"][\"split\"]\n",
    "        if split_args is None:\n",
    "            raise ValueError(\"The split_args in eval_args should not be None.\")\n",
    "        if not isinstance(split_args, dict):\n",
    "            raise ValueError(f\"The split_args [{split_args}] should be a dict.\")\n",
    "\n",
    "        split_mode = list(split_args.keys())[0]\n",
    "        assert len(split_args.keys()) == 1\n",
    "        if split_mode != \"CO\":\n",
    "            raise NotImplementedError(\"The split_mode must be 'CO'.\")\n",
    "        elif split_mode == \"CO\":\n",
    "            cutoff = split_args[\"RS\"]\n",
    "            # NOTE: HoangLe [Jun-05]: cutoff may come with different types: string, int\n",
    "\n",
    "            group_by = self.config[\"eval_args\"][\"group_by\"]\n",
    "            datasets = self.split_by_cuttoff(cutoff=cutoff, group_by=group_by)\n",
    "    \n",
    "        \n",
    "        return datasets\n",
    "\n",
    "    def split_by_cuttoff(self, cutoff: str|int, group_by: str) -> list[Dataset]:\n",
    "        \"\"\"Split the interations by cutoff date\n",
    "\n",
    "        Args:\n",
    "            cutoff (str | int): cutoff date in Unix timestamp format\n",
    "            group_by (str): field to group by, usually the user_id\n",
    "\n",
    "        Returns:\n",
    "            list[Dataset]: list of training/validation/testing dataset, whose interaction features has been split.\n",
    "\n",
    "        Notes:\n",
    "            cutoff may be different types: string of Unix timestamp (e.g. '1717923174'), integer of Unix timestamp (e.g. 1717923174)\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: HoangLe [Jun-05]: Implement this, may follow method split_by_ratio()\n",
    "        \n",
    "        self.logger.debug(f\"split by cutoff date = '{cutoff}', group_by=[{group_by}]\")\n",
    "\n",
    "        # Convert cutoff to suitable format and apply 0-1 normalization with max/min timestamp\n",
    "        if isinstance(cutoff, str):\n",
    "            cutoff_conv = float(cutoff)\n",
    "        else:\n",
    "            cutoff_conv = float(cutoff)\n",
    "\n",
    "        def norm_timestamp(timestamp: float):\n",
    "            mx, mn = self.timestamp_max, self.timestamp_min\n",
    "            if mx == mn:\n",
    "                self.logger.warning(\n",
    "                    f\"All the same value in [{field}] from [{feat}_feat].\"\n",
    "                )\n",
    "                arr = 1.0\n",
    "            else:\n",
    "                arr = (timestamp - mn) / (mx - mn)\n",
    "            return arr\n",
    "\n",
    "        cutoff_conv = norm_timestamp(cutoff_conv)\n",
    "            \n",
    "\n",
    "        grouped_inter_feat_index = self._grouped_index(\n",
    "            self.inter_feat[group_by].numpy()\n",
    "        )\n",
    "\n",
    "        next_index = [[]]*3     # 'next_index' contains the indices for training/validation/testing dataset\n",
    "        for grouped_index in grouped_inter_feat_index:\n",
    "            # Split the grouped_index into into train/validation/test\n",
    "\n",
    "            train_indices, val_indices, test_indices = [], [], []\n",
    "\n",
    "            ## TODO: HoangLe [Jun-05]: Investivate how to access 'timestamp' and how to split the self.inter_feat using cutoff\n",
    "            split_ids = self._calcu_split_ids(tot=tot_cnt, ratios=ratios)\n",
    "            for index, start, end in zip(\n",
    "                next_index, [0] + split_ids, split_ids + [tot_cnt]\n",
    "            ):\n",
    "                index.extend(grouped_index[start:end])\n",
    "\n",
    "        self._drop_unused_col()\n",
    "        next_df = [self.inter_feat[index] for index in next_index]\n",
    "        next_ds = [self.copy(_) for _ in next_df]\n",
    "        return next_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(config):\n",
    "    \"\"\"Create dataset according to :attr:`config['model']` and :attr:`config['MODEL_TYPE']`.\n",
    "    If :attr:`config['dataset_save_path']` file exists and\n",
    "    its :attr:`config` of dataset is equal to current :attr:`config` of dataset.\n",
    "    It will return the saved dataset in :attr:`config['dataset_save_path']`.\n",
    "\n",
    "    Args:\n",
    "        config (Config): An instance object of Config, used to record parameter information.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Constructed dataset.\n",
    "    \"\"\"\n",
    "    dataset_module = importlib.import_module(\"recbole.data.dataset\")\n",
    "    if hasattr(dataset_module, config[\"model\"] + \"Dataset\"):\n",
    "        dataset_class = getattr(dataset_module, config[\"model\"] + \"Dataset\")\n",
    "    else:\n",
    "        model_type = config[\"MODEL_TYPE\"]\n",
    "        type2class = {\n",
    "            ModelType.GENERAL: \"Dataset\",\n",
    "            ModelType.SEQUENTIAL: \"SequentialDataset\",\n",
    "            ModelType.CONTEXT: \"Dataset\",\n",
    "            ModelType.KNOWLEDGE: \"KnowledgeBasedDataset\",\n",
    "            ModelType.TRADITIONAL: \"Dataset\",\n",
    "            ModelType.DECISIONTREE: \"Dataset\",\n",
    "            # TODO: HoangLe [Jun-09]: Continue the below\n",
    "            ModelType: \"TimeCutoffDataset\"\n",
    "        }\n",
    "        dataset_class = getattr(dataset_module, type2class[model_type])\n",
    "\n",
    "    default_file = os.path.join(\n",
    "        config[\"checkpoint_dir\"], f'{config[\"dataset\"]}-{dataset_class.__name__}.pth'\n",
    "    )\n",
    "    file = config[\"dataset_save_path\"] or default_file\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "        dataset_args_unchanged = True\n",
    "        for arg in dataset_arguments + [\"seed\", \"repeatable\"]:\n",
    "            if config[arg] != dataset.config[arg]:\n",
    "                dataset_args_unchanged = False\n",
    "                break\n",
    "        if dataset_args_unchanged:\n",
    "            logger = getLogger()\n",
    "            logger.info(set_color(\"Load filtered dataset from\", \"pink\") + f\": [{file}]\")\n",
    "            return dataset\n",
    "\n",
    "    dataset = dataset_class(config)\n",
    "    if config[\"save_dataset\"]:\n",
    "        dataset.save()\n",
    "    return dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
