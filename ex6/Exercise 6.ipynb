{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0907d463",
   "metadata": {},
   "source": [
    "## Student: Huy Hoang Le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3611ce15",
   "metadata": {},
   "source": [
    "## Exercise 6.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71003880",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "1. A typical layer of CNN consists of 3 components in order:\n",
    "    \n",
    "1st layer: convolution\n",
    "It's used to extract feature. Normally, the kernel has odd size and usually there are multiple filters whose results are stacked. The usual available hyperparams are: kernel_size, dilation, padding, stride, no. filters.\n",
    "\n",
    "2nd layer: activation function\n",
    "It's used as detector stage. Normal option is ReLU, LeakyReLU, GELU. Tanh is not usually used as activation function for CNN but rather for fully connected NN.\n",
    "\n",
    "3rd layer: pooling\n",
    "It is used to avoid overfitting. In particular, it reduces the activation maps, which reduces the complexity.\n",
    "\n",
    "2. Optimization strategies\n",
    "- increase batch size in minibatch training: GPU can handle in parallel multiple images\n",
    "  -> Make the train faster\n",
    "- shuffle in training: shuffle the whole training set every epoch\n",
    "  -> reduce variance and ensure the generalization of the model\n",
    "- residual layer: add skip connection in fully connected layer\n",
    "  -> reduce vanishing gradient\n",
    "- data augmentation: apply different augmentation techniques (flipping, rotation, zooming, shearing, etc.) to the original images\n",
    "  -> Increase model robustness, generalization and partly deal with the case of data shortage\n",
    "- image normalization: done in transforming step\n",
    "  -> eliminate difference in brightness and constrast among the training samples, reduce the outliers' impact, ensure training stability and speed up training process\n",
    "- batch norm: added right before the next layer\n",
    "  -> speed up training process, deal with covariate shift between training and testing samples\n",
    "- transfer learning: used pretrained weights of one or several layers of architecture\n",
    "  -> can utilize the weights trained with huge datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9feb82c",
   "metadata": {},
   "source": [
    "## Exercise 6.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8600a3ae",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "1. Why is the network called U-net?\n",
    "\n",
    "In section Introduction, the authors mention that the expansive path (upsampling flow) is symmetric to contracting path (downsampling flow), which yields a u-shaped architecture.\n",
    "\n",
    "2. Why does the paper work for small dataset?\n",
    "\n",
    "In order to make the system work for small dataset, the authors mention in Introduction section that they employs excesive data augmentation by applying elastic deformation to training images. This technique allows the network to learn invariance. The authors also explain that this invariance is important in biomedial segmentation since deformation is the most common variation in tissue and realistic deformations can be simulated efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
