{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hoanghu/projects/Thesis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoanghu/projects/Thesis/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from logging import getLogger\n",
    "\n",
    "import yaml\n",
    "from recbole.config import Config\n",
    "from recbole.data import data_preparation, create_dataset\n",
    "from recbole.trainer import HyperTuning\n",
    "from recbole.utils import (\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    ModelType\n",
    ")\n",
    "\n",
    "import src.utils as utils\n",
    "from src.real_temporal import SimulatedOnlineSequentialDataset, SimulatedOnlineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "use_cutoff = False\n",
    "test_inactive = True\n",
    "\n",
    "model_name = \"NPE\"\n",
    "loss_type = \"CE\"\n",
    "# dataset_name = \"amazon-digital-music\"\n",
    "# cutoff_time = \"1403568000\"\n",
    "\n",
    "dataset_name = \"ml-1m\"\n",
    "cutoff_time = \"976324045\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = utils.Paths(model_name, dataset_name, use_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    # For model \n",
    "    'model': model_name,\n",
    "    'loss_type': loss_type,\n",
    "\n",
    "    # For data\n",
    "    'dataset': dataset_name, \n",
    "    'load_col': {\"inter\": ['user_id', 'item_id', 'timestamp']},\n",
    "    'use_cutoff': use_cutoff,\n",
    "    'normalize_all': False,\n",
    "    'user_inter_num_interval': \"[10,inf)\",\n",
    "\n",
    "    # For training\n",
    "    'epochs': 20,\n",
    "    'train_batch_size': 4096,\n",
    "    'eval_step': 1,\n",
    "    'stopping_step': 3,\n",
    "    'learning_rate': 1e-3,\n",
    "    \n",
    "    # For evaluation\n",
    "    'eval_batch_size': 4096,\n",
    "    'metrics': [\"NDCG\", \"Precision\", \"Recall\", \"MRR\", \"Hit\", \"MAP\"],\n",
    "    'topk': 10,\n",
    "    'valid_metric': 'NDCG@10',\n",
    "\n",
    "    # Environment\n",
    "    'gpu_id': 0,\n",
    "    \"seed\": seed,\n",
    "    \"reproducibility\": True,\n",
    "    'device': 'cuda',\n",
    "    'use_gpu': True,\n",
    "    'data_path': paths.get_path_data_raw(),\n",
    "    \"checkpoint_dir\": paths.get_path_dir_ckpt(),\n",
    "    \"show_progress\": True,\n",
    "    'save_dataset': True,\n",
    "    'dataset_save_path': paths.get_path_data_processed(),\n",
    "    'save_dataloaders': True,\n",
    "    'dataloaders_save_path': paths.get_path_dataloader(),\n",
    "}\n",
    "\n",
    "if use_cutoff is True:\n",
    "    config_dict['eval_args'] = {\n",
    "        \"order\": \"TO\",\n",
    "        \"split\": {\"CO\": cutoff_time},\n",
    "        \"group_by\": 'user_id',\n",
    "        'mode': 'full'\n",
    "    }\n",
    "else:\n",
    "    config_dict['eval_args'] = {\n",
    "        \"order\": \"TO\",\n",
    "        \"split\": { \"LS\": \"valid_and_test\" },\n",
    "        \"group_by\": None,\n",
    "        'mode': 'full'\n",
    "    }\n",
    "\n",
    "if loss_type == \"CE\":\n",
    "    config_dict[\"train_neg_sample_args\"] = None\n",
    "else:\n",
    "    config_dict[\"train_neg_sample_args\"] = {\n",
    "        \"distribution\": \"uniform\",\n",
    "        \"sample_num\": 1,\n",
    "        # \"dynamic\": False,\n",
    "        # \"candidate_num\": 0,\n",
    "    }\n",
    "\n",
    "config = Config(\n",
    "    model_name,\n",
    "    dataset_name,\n",
    "    config_dict=config_dict,\n",
    "    config_file_list=[paths.get_path_param_conf()],\n",
    ")\n",
    "\n",
    "with open(paths.get_path_conf(), 'w+') as f:\n",
    "    yaml.dump(config.external_config_dict, f, allow_unicode=True)\n",
    "\n",
    "init_seed(config[\"seed\"], config[\"reproducibility\"])\n",
    "utils.init_logger(config, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06 Aug 14:18    INFO  Saving filtered dataset into [logs/Aug06_141802_NPE_ml-1m_usecutoff_False/ckpts/ml-1m-SequentialDataset.pth]\n"
     ]
    }
   ],
   "source": [
    "# Define data related things\n",
    "if use_cutoff is True:\n",
    "    match (config[\"MODEL_TYPE\"]):\n",
    "        case ModelType.GENERAL | ModelType.CONTEXT | ModelType.TRADITIONAL:\n",
    "            ds = \"SimulatedOnlineDataset\"\n",
    "        case ModelType.SEQUENTIAL:\n",
    "            ds = \"SimulatedOnlineSequentialDataset\"\n",
    "        case _:\n",
    "            print(f\"model type: {config['MODEL_TYPE']}\")\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    dataset = eval(ds)(config)\n",
    "else:\n",
    "    dataset = create_dataset(config)\n",
    "\n",
    "# if separate_activeness is True:\n",
    "#     utils.remove_inactive(dataset, cutoff=cutoff_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = cutoff_time\n",
    "\n",
    "if not isinstance(cutoff, float):\n",
    "    cutoff = float(cutoff)\n",
    "\n",
    "feat = dataset.inter_feat\n",
    "\n",
    "# Determine min/max timestamp for each user\n",
    "timestamp_byuser = feat.groupby(\"user_id\")[\"timestamp\"]\n",
    "min_ts = (\n",
    "    timestamp_byuser.min().reset_index().rename(columns={\"timestamp\": \"min_ts\"})\n",
    ")\n",
    "max_ts = (\n",
    "    timestamp_byuser.max().reset_index().rename(columns={\"timestamp\": \"max_ts\"})\n",
    ")\n",
    "user = min_ts.merge(max_ts, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "# Determine inactive users using given cutoff\n",
    "condition_active_user = (user[\"min_ts\"] <= cutoff) & (cutoff <= user[\"max_ts\"])\n",
    "user_inactive = user[~condition_active_user]['user_id']\n",
    "user_active = user[condition_active_user]['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_active = feat[feat['user_id'].isin(user_active)].copy()\n",
    "feat_inactive = feat[feat['user_id'].isin(user_inactive)].copy()\n",
    "\n",
    "dataset_active = dataset.copy(feat_active)\n",
    "dataset_inactive = dataset.copy(feat_inactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06 Aug 14:34    INFO  Saving split dataloaders into: [logs/Aug06_141802_NPE_ml-1m_usecutoff_False/ckpts/ml-1m-for-NPE-dataloader.pth]\n",
      "06 Aug 14:34    INFO  [Training]: train_batch_size = [4096] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\n",
      "06 Aug 14:34    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': None, 'mode': {'valid': 'full', 'test': 'full'}}]\n",
      "06 Aug 14:34    INFO  Saving split dataloaders into: [logs/Aug06_141802_NPE_ml-1m_usecutoff_False/ckpts/ml-1m-for-NPE-dataloader.pth]\n",
      "06 Aug 14:34    INFO  [Training]: train_batch_size = [4096] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\n",
      "06 Aug 14:34    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': None, 'mode': {'valid': 'full', 'test': 'full'}}]\n",
      "06 Aug 14:35    INFO  Saving split dataloaders into: [logs/Aug06_141802_NPE_ml-1m_usecutoff_False/ckpts/ml-1m-for-NPE-dataloader.pth]\n",
      "06 Aug 14:35    INFO  [Training]: train_batch_size = [4096] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\n",
      "06 Aug 14:35    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': None, 'mode': {'valid': 'full', 'test': 'full'}}]\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, _ = data_preparation(config, dataset)\n",
    "_, _, test_data_active = data_preparation(config, dataset_active)\n",
    "_, _, test_data_inactive = data_preparation(config, dataset_inactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1190\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "users = []\n",
    "for x in test_data_active:\n",
    "    users.extend(x[0]['user_id'].tolist())\n",
    "\n",
    "print(len(users))\n",
    "\n",
    "users_active = set(user_active.tolist())\n",
    "users = set(users)\n",
    "\n",
    "print(users.difference(users_active))\n",
    "print(users_active.difference(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06 Aug 14:15    INFO  Saving split dataloaders into: [logs/Aug06_141311_NPE_ml-1m_usecutoff_False/ckpts/ml-1m-for-NPE-dataloader.pth]\n",
      "06 Aug 14:15    INFO  [Training]: train_batch_size = [4096] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\n",
      "06 Aug 14:15    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': None, 'mode': {'valid': 'full', 'test': 'full'}}]\n"
     ]
    }
   ],
   "source": [
    "# # Define model\n",
    "# model_name = config['model']\n",
    "# model = get_model(model_name)(config, train_data._dataset).to(config['device'])\n",
    "\n",
    "# # Define trainer\n",
    "# trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
