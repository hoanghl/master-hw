{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d953bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6598d929-2af6-4d94-acee-1341af2a1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size\n",
    "N = 5\n",
    "D = 6\n",
    "\n",
    "X = [[0.7052993232951105, 2.698486968663134, 0.06130004139463617, 1.1034367343425524, -0.747867633122564], [-0.751156768238722, 0.12697248593333335, 2.5764048596077758, 1.5158804193727737, -0.7249906432883477], [-1.1970557609782755, 1.6197212631312385, -0.05254225954756783, 0.9904539073417479, 0.6649690313693303], [-1.0385218121268096, 1.8037910634543346, -1.2907609517028438, -0.47264614920154363, -1.5232527583532625], [-0.03388858727378599, 1.539108034371443, -0.48926774665292694, 0.4494502269182922, -0.8288869288353324], [-0.28498992240722576, 0.321731789914264, -0.6583006110725065, 0.3530802127275212, 0.9533629092311684]]\n",
    "Wq = [[-1.6915835622814102, 1.6168851803699098, 0.9005298734799696, -0.5435904829764218, 0.3529689575207902, -0.9619799935341771], [-0.3857147673162622, 0.9513307245529884, -0.28804816487908663, 1.0355324398968084, 0.4546644592016658, 1.139297763199957], [0.3774588388290535, -0.8738578786476723, -0.9937645295151809, 0.505984012956844, -1.420800467209645, 0.009892908375293762], [0.2577602852869834, 1.44866422319851, -1.2000035068387314, 0.18281166451593303, 0.12655309651231234, 1.578765173240208], [-0.7745689981464058, 0.8227442221776315, -0.7255888420216551, -1.3184265784969222, 0.2962382374907567, 0.7597655400750712], [1.0865245377523511, 0.32070497820403776, -1.518406018750088, -2.2556501487125966, 2.1799172739495862, -0.7178248900219233]]\n",
    "Wk = [[0.31593300374401917, -0.365279495429932, -1.2965454363562494, 0.27542867678035476, -1.737168891087042, 1.0780438339601688], [-2.3442981921148096, -1.1101235287988405, 0.6306997676455754, -1.2385519148539723, 2.202882286663097, 0.2935433100425166], [1.11418783440032, -0.36231494120079394, -0.4687673027654192, 1.9252734140173828, -1.081573738529366, -1.1736062101034812], [-0.39412420064761666, 1.0086840602465978, -1.6824722532330467, -0.04300668104550703, -3.2852877292643847, -1.3545308782530943], [-0.9481255084787834, -1.1355601278710354, -0.9640481752200648, 1.4395065737034969, 1.2802888676276918, 1.1978256257300115], [-0.7044944066408009, 0.3617312606835614, 0.40353330748680905, -1.3885375565996605, -0.20783947627000213, -0.5290736470160097]]\n",
    "Wv = [[-0.07119202180795095, 0.7303163324088124, 1.0491765285789945, -0.06618134174990031, 1.6330602057337622, 0.8642022107016261], [0.4081425173675768, -1.0259835303572715, -0.7153342894013545, -0.6316485947803749, -0.8827310523169841, -0.0885512548113335], [-0.35577423606297215, 0.5499650341878631, -1.406703550086041, 0.06554823639406983, 0.624624318901869, 0.3732726349708465], [1.395153272770115, -1.260727999554377, -1.2726566639407204, -0.5615414087426772, 1.6239403855966406, -0.21666139010775304], [-0.4388394531419344, -0.5647367190069518, -1.4337808942746009, -0.9557795952304533, 0.3610975369273206, -0.7870863143724625], [0.23746905712785424, 0.49091607456131625, 0.40162634225346283, -0.5298843730022454, 1.4413190345501548, 2.2973435889337597]]\n",
    "\n",
    "X = np.array(X)\n",
    "Wq = np.array(Wq)\n",
    "Wk = np.array(Wk)\n",
    "Wv = np.array(Wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b382714-52d9-4a53-9361-19e81d5b64c6",
   "metadata": {},
   "source": [
    "### (a) Implement the self-attention operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0314ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X: np.ndarray, dim: int = 0):\n",
    "    ex = np.exp(X - X.max(axis=dim, keepdims=True))\n",
    "    softmax = ex / ex.sum(axis=dim, keepdims=True)\n",
    "\n",
    "    return softmax\n",
    "\n",
    "def self_attention(X, Wq, Wk, Wv):\n",
    "    Q, K, V = Wq @ X, Wk @ X, Wv @ X\n",
    "\n",
    "    attention_weights = softmax((Q.T @ K) / np.sqrt(D))\n",
    "    output = V @ attention_weights\n",
    "    \n",
    "    return output, attention_weights\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4298990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output:\n",
      " [[ 0.7 -0.2  4.3  4.3  2.1]\n",
      " [-1.3  1.6 -2.7 -2.7 -1.7]\n",
      " [ 0.7 -1.3 -2.  -2.  -0. ]\n",
      " [-2.8 -1.7  3.   3.  -1.5]\n",
      " [-0.   0.2 -5.  -5.  -1.4]\n",
      " [ 0.   1.5  3.4  3.4  1.7]]\n",
      "Self-Attention Matrix:\n",
      " [[4.4e-03 1.6e-03 6.5e-04 9.9e-04 1.3e-05]\n",
      " [2.2e-03 6.7e-05 1.0e+00 1.0e+00 4.7e-02]\n",
      " [9.1e-01 9.7e-15 4.2e-06 6.2e-09 4.3e-01]\n",
      " [8.1e-02 6.4e-12 3.8e-04 4.7e-06 5.2e-01]\n",
      " [5.0e-03 1.0e+00 1.7e-09 1.8e-05 3.3e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the output\n",
    "output, attention_weights = self_attention(X, Wq, Wk, Wv)\n",
    "\n",
    "# Print in a nice format\n",
    "np.set_printoptions(precision=1)\n",
    "print(\"Self-Attention Output:\\n\", output)\n",
    "print(\"Self-Attention Matrix:\\n\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644c648-2f0a-4ae2-a50e-17d0c7f412bb",
   "metadata": {},
   "source": [
    "### (b) Implement multi-head attention, using the previously implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e149e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, Wq, Wk, Wv, H):\n",
    "    split = D // H\n",
    "    outputs = []\n",
    "    # attn_weights = []\n",
    "\n",
    "    for i in range(0, D, split):\n",
    "        Wq_split, Wk_split, Wv_split = Wq[i:i + split], Wk[i:i + split], Wv[i:i + split],\n",
    "    \n",
    "        out_split, attn_weights_split = self_attention(X, Wq_split, Wk_split, Wv_split)\n",
    "\n",
    "        outputs.append(out_split)\n",
    "        # attn_weights.append(attn_weights_split)\n",
    "\n",
    "    output = np.concatenate(outputs, axis=0)\n",
    "    # attention_weights = np.concatenate(attn_weights, axis=0)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5563ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output:\n",
      " [[ 0.7 -2.  -1.7 -1.3  1.5]\n",
      " [-1.1  2.6  2.3  2.  -1.5]\n",
      " [ 0.  -0.7  0.4 -1.3 -0.5]\n",
      " [-1.   0.1 -2.3  0.9 -0.6]\n",
      " [-1.6 -0.  -5.  -5.   1.8]\n",
      " [ 1.7  1.6  3.4  3.3 -0.2]]\n"
     ]
    }
   ],
   "source": [
    "# Compute multi-head attention\n",
    "H = 3\n",
    "# B = np.random.random((D, D))\n",
    "B = np.identity(D)\n",
    "attention_output = multi_head_attention(X, Wq, Wk, Wv, H)\n",
    "output = B @ attention_output\n",
    "# Again print the requested results\n",
    "\n",
    "# Print in a nice format\n",
    "np.set_printoptions(precision=1)\n",
    "print(\"Self-Attention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c2abb-23bf-4772-9090-420aafac4639",
   "metadata": {},
   "source": [
    "### (c+d) Provide the answers/explanations requested in the problem sheet:\n",
    "1. Why the results are different?\n",
    "2. What happens if you change the order of two inputs="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ce84d",
   "metadata": {},
   "source": [
    "#### HoangLe: For c, please visit the PDF report file for the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7db2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7,  2.7,  0.1,  1.1, -0.7],\n",
       "       [-0.8,  0.1,  2.6,  1.5, -0.7],\n",
       "       [-1.2,  1.6, -0.1,  1. ,  0.7],\n",
       "       [-1. ,  1.8, -1.3, -0.5, -1.5],\n",
       "       [-0. ,  1.5, -0.5,  0.4, -0.8],\n",
       "       [-0.3,  0.3, -0.7,  0.4,  1. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948a60e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.7,  0.7,  0.1,  1.1, -0.7],\n",
       "       [ 0.1, -0.8,  2.6,  1.5, -0.7],\n",
       "       [ 1.6, -1.2, -0.1,  1. ,  0.7],\n",
       "       [ 1.8, -1. , -1.3, -0.5, -1.5],\n",
       "       [ 1.5, -0. , -0.5,  0.4, -0.8],\n",
       "       [ 0.3, -0.3, -0.7,  0.4,  1. ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X.copy()\n",
    "\n",
    "X_new[:, 0] = X[:, 1].copy()\n",
    "X_new[:, 1] = X[:, 0].copy()\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "435b0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-6\n",
    "\n",
    "for h in range(H):\n",
    "    output = multi_head_attention(X, Wq, Wk, Wv, H)\n",
    "    output_new = multi_head_attention(X_new, Wq, Wk, Wv, H)\n",
    "\n",
    "    # Check if 2 first columns of the output are swapped and the others stay the same\n",
    "    assert np.linalg.norm(output[:, 0] - output_new[:, 1]) < EPS\n",
    "    assert np.linalg.norm(output[:, 1] - output_new[:, 0]) < EPS\n",
    "\n",
    "    for j in range(2, N):\n",
    "        assert np.linalg.norm(output[:, j] - output_new[:, j]) < EPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00359ad3",
   "metadata": {},
   "source": [
    "#### HoangLe: For d, I conclude that in every cases of H, if the input' columns X are swapped, the output are swapped accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
