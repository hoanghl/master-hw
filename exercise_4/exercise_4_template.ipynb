{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598d929-2af6-4d94-acee-1341af2a1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size\n",
    "N = 5\n",
    "D = 6\n",
    "\n",
    "X = [[0.7052993232951105, 2.698486968663134, 0.06130004139463617, 1.1034367343425524, -0.747867633122564], [-0.751156768238722, 0.12697248593333335, 2.5764048596077758, 1.5158804193727737, -0.7249906432883477], [-1.1970557609782755, 1.6197212631312385, -0.05254225954756783, 0.9904539073417479, 0.6649690313693303], [-1.0385218121268096, 1.8037910634543346, -1.2907609517028438, -0.47264614920154363, -1.5232527583532625], [-0.03388858727378599, 1.539108034371443, -0.48926774665292694, 0.4494502269182922, -0.8288869288353324], [-0.28498992240722576, 0.321731789914264, -0.6583006110725065, 0.3530802127275212, 0.9533629092311684]]\n",
    "Wq = [[-1.6915835622814102, 1.6168851803699098, 0.9005298734799696, -0.5435904829764218, 0.3529689575207902, -0.9619799935341771], [-0.3857147673162622, 0.9513307245529884, -0.28804816487908663, 1.0355324398968084, 0.4546644592016658, 1.139297763199957], [0.3774588388290535, -0.8738578786476723, -0.9937645295151809, 0.505984012956844, -1.420800467209645, 0.009892908375293762], [0.2577602852869834, 1.44866422319851, -1.2000035068387314, 0.18281166451593303, 0.12655309651231234, 1.578765173240208], [-0.7745689981464058, 0.8227442221776315, -0.7255888420216551, -1.3184265784969222, 0.2962382374907567, 0.7597655400750712], [1.0865245377523511, 0.32070497820403776, -1.518406018750088, -2.2556501487125966, 2.1799172739495862, -0.7178248900219233]]\n",
    "Wk = [[0.31593300374401917, -0.365279495429932, -1.2965454363562494, 0.27542867678035476, -1.737168891087042, 1.0780438339601688], [-2.3442981921148096, -1.1101235287988405, 0.6306997676455754, -1.2385519148539723, 2.202882286663097, 0.2935433100425166], [1.11418783440032, -0.36231494120079394, -0.4687673027654192, 1.9252734140173828, -1.081573738529366, -1.1736062101034812], [-0.39412420064761666, 1.0086840602465978, -1.6824722532330467, -0.04300668104550703, -3.2852877292643847, -1.3545308782530943], [-0.9481255084787834, -1.1355601278710354, -0.9640481752200648, 1.4395065737034969, 1.2802888676276918, 1.1978256257300115], [-0.7044944066408009, 0.3617312606835614, 0.40353330748680905, -1.3885375565996605, -0.20783947627000213, -0.5290736470160097]]\n",
    "Wv = [[-0.07119202180795095, 0.7303163324088124, 1.0491765285789945, -0.06618134174990031, 1.6330602057337622, 0.8642022107016261], [0.4081425173675768, -1.0259835303572715, -0.7153342894013545, -0.6316485947803749, -0.8827310523169841, -0.0885512548113335], [-0.35577423606297215, 0.5499650341878631, -1.406703550086041, 0.06554823639406983, 0.624624318901869, 0.3732726349708465], [1.395153272770115, -1.260727999554377, -1.2726566639407204, -0.5615414087426772, 1.6239403855966406, -0.21666139010775304], [-0.4388394531419344, -0.5647367190069518, -1.4337808942746009, -0.9557795952304533, 0.3610975369273206, -0.7870863143724625], [0.23746905712785424, 0.49091607456131625, 0.40162634225346283, -0.5298843730022454, 1.4413190345501548, 2.2973435889337597]]\n",
    "\n",
    "X = np.array(X)\n",
    "Wq = np.array(Wq)\n",
    "Wk = np.array(Wk)\n",
    "Wv = np.array(Wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b382714-52d9-4a53-9361-19e81d5b64c6",
   "metadata": {},
   "source": [
    "### (a) Implement the self-attention operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X, Wq, Wk, Wv):\n",
    "    ...    \n",
    "    return output, attention_weights\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the output\n",
    "output, attention_weights = self_attention(X, Wq, Wk, Wv)\n",
    "\n",
    "# Print in a nice format\n",
    "np.set_printoptions(precision=1)\n",
    "print(\"Self-Attention Output:\\n\", output)\n",
    "print(\"Self-Attention Matrix:\\n\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644c648-2f0a-4ae2-a50e-17d0c7f412bb",
   "metadata": {},
   "source": [
    "### (b) Implement multi-head attention, using the previously implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, Wq, Wk, Wv, H):\n",
    "    ...\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute multi-head attention\n",
    "H = 3\n",
    "attention_output = multi_head_attention(X, Wq, Wk, Wv, H)\n",
    "# Again print the requested results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c2abb-23bf-4772-9090-420aafac4639",
   "metadata": {},
   "source": [
    "### (c+d) Provide the answers/explanations requested in the problem sheet:\n",
    "1. Why the results are different?\n",
    "2. What happens if you change the order of two inputs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7db2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
